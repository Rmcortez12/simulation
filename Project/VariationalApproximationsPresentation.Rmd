---
title: "Explaining Variational Approximations"
author: "Ricardo Cortez & Ben Graf"
date: "28 Apr 2021"
output:
  beamer_presentation:
    theme: CambridgeUS
    colortheme: default
    fonttheme: structurebold
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
pacman::p_load(pacman)
```

## Outline

- Introduction
<p>&nbsp;</p>
- Density Transform Approach
<p>&nbsp;</p>
- Tangent Transform Approach
<p>&nbsp;</p>
- Conclusions

## Introduction
NEED STUFF HERE! VERY IMPORTANT!

## Density Transform Approach

- Approximates intractable posterior densities with better known and easier to deal with densities
- Guided by Kullback-Leibler divergence
$$\text{formula 2+3+4+ } p(y) > p(y;q)$$
  - provides a lower bound which can be maximized in order to minimize the k-l divergence between q and p(.|y)
- Two main types of restrictions for the $q$ density:
  + Product Density Transforms (non-parametric)
  + Parametric Density Transforms (parametric)

## Product Density Transforms

- Suppose $q$ is subject to the product restriction from the previous slide
- It can be shown that the optimal densities satisfy 
$$\text{formula 5}$$
where $E_{-\boldsymbol{\theta}_i}$ denotes the expectation of the density with $q_i$ removed
- This leads to the algorithm on the next slide to solve for the $q_i^*$
- Notes:
  + Can show that convergence to at least local optima guaranteed
  + If conjugate priors used, then the $q_i^*$ updates reduce to updating parameters in a density family
  + Common to monitor convergence using $\text{log }\underline{p}(\boldsymbol{y};q)$

## Product Density Transforms

SHOW ALGORITHM 1

## Connection with Gibbs Sampling

- An alternative expression for the $q_i^*$ is 
$$\text{formula 6 subbing in "rest"}$$
- The distributions $\boldsymbol{\theta}_i|\text{sub in for "rest"}$ are called full conditionals in Markov Chain Monte Carlo
- Gibbs sampling uses repeated draws from these
- In fact, product density transforms and Gibbs are tractable in the same scenarios

## Product DT Example 1: Normal Random Sample

- Objective is to approximate Bayesian inference for a random sample from a Normal distribution
$$\text{first equation}$$
with conjugate priors
$$\text{second equation}$$
- The product density transform approximation of $p(\mu,\sigma^2|\boldsymbol{x})$ is 
$$\text{formula 8}$$
- The optimal densities take the form
$$\text{next equation}$$

## Product DT Example 1: Normal Random Sample

- The resulting estimates are:
$$q_{\sigma^2}^*(\sigma^2)\text{ is InverseGamma(...)}$$
$$q_{\mu}^*(\mu)\text{ is Normal(...)}$$
and
$$\text{log }\underline{p}(\boldsymbol{x};q) = ...$$
- This leads to the algorithm on the next slide

## Product DT Example 1: Normal Random Sample

SHOW ALGORITHM 2

## Product DT Example 1: Normal Random Sample

- Upon convergence, the posterior densities are approximated as
$$p(\mu|\boldsymbol{x}) \approx ...$$
$$p(\sigma^2|\boldsymbol{x}) \approx ...$$
- Next slide's plots compare product density variational approximations with exact posterior density
  + Sample size $n=20$ from $N(100,225)$
  + Vague priors chosen: $\mu \sim N(0,10^8),\ \sigma^2 \sim IG(\frac{1}{100},\frac{1}{100})$
  + Initial value $B_{q(\sigma^2)}=1$
  + Convergence very rapid, accuracy quite good

## Product DT Example 1: Normal Random Sample

SHOW FIGURE 2
```{r echo=FALSE, out.width='70%', fig.align='center'}
#knitr::include_graphics('Table of selected variables.png')
```

## Product DT Example 2: Linear Mixed Model

- Objective is to approximate Bayesian inference for a random sample from a Gaussian linear mixed model
$$\text{formula 11}$$
where 
  + $\boldsymbol{y}$ is $n \times 1$ response, 
  + $\boldsymbol{\beta}$ is $p \times 1$ fixed effects, 
  + $\boldsymbol{u}$ is random effects,
  + $\boldsymbol{X}$ and $\boldsymbol{Z}$ are design matrices, and
  + $\boldsymbol{G}$ and $\boldsymbol{R}$ are covariance matrices
- Conjugate priors are
$$\text{formula 13}$$

## Product DT Example 2: Linear Mixed Model

- The two-component product transform is
$$\text{formula 14}$$
- This leads to optimal densities
$$q_{\boldsymbol{\beta},\boldsymbol{u}}^*({\boldsymbol{\beta},\boldsymbol{u}})\text{ is a Multivariate Normal density}$$
$$q_{\boldsymbol{\sigma^2}}^*\text{ is a product of $r+1$ Inverse Gamma densities}$$
and
$$\text{log }\underline{p}(\boldsymbol{y};q) = ...$$
- This leads to the algorithm on the next slide

## Product DT Example 2: Linear Mixed Model

SHOW ALGORITHM 3

## Product DT Example 2: Linear Mixed Model

- Upon convergence, the posterior densities are approximated as
$$p(\boldsymbol{\beta},\boldsymbol{u}|\boldsymbol{y}) \approx ...$$
$$p(\sigma^2_{u1},...,\sigma^2_{ur},\sigma^2_{\varepsilon}|\boldsymbol{y}) \approx ...$$

## Product DT Example 2: Linear Mixed Model

- Next slide's plots compare product density variational approximations with exact posterior density
  + Data set is longitudinal orthodontic measurements (Pinheiro & Bates, 2000)
  + Random intercept model:
$$\text{formulas 16}$$
  + Vague priors chosen: $\sigma^2_\beta = 10^8,\ A=B=\frac{1}{100}$
  + Compared against kernel density estimates using 1M MCMC samples
  + Convergence again quite rapid, estimates quite close to MCMC, statistical significance of all parameters

## Product DT Example 2: Linear Mixed Model

SHOW FIGURE 4
```{r echo=FALSE, out.width='70%', fig.align='center'}
#knitr::include_graphics('Table of selected variables.png')
```

## Parametric DT Example: Poisson Regression

- Now assume $q$ is subject to the parametric restriction
  - Belongs to a specific parametric family that (hopefully) results in a more tractable approximation to the posterior density
- Poisson Regression with Gaussian Transform example: 
  - Consider the Bayesian Poisson Regression Model: 
  
  $$Y_i|\beta_0...$$
  - With priors on the coefficient vector of $\beta$ ~ $N(\mu_\beta,\Sigma_\beta)$
  
## Parametric DT Example: Poisson Regression

- The likelihood is

$$p(y|\beta)=$$

- This leads to an integral that has no closed form solution (intractable):

$$\text{marginal likelihood}$$

## Parametric DT Example: Poisson Regression

- Take q ~ N()
$$q() = equation$$
- Then the lower bound as defined earlier gives explicitly
$$eq19$$

- From earlier,
$$logp(y) \ge log(lowerbound)$$
- The optimal variational parameters are found through maximizing this inequality using Newton-Raphson iteration
- This minimizes the K-L divergence and provides the optimal Gaussian density transform $q^*$

## Tangent Transform Approach
  
- Not all variational approximations fit into Kullback-Leibler divergence framework
- Tangent transforms work with *tangent-type* representations of concave/convex functions
- eq 20 
<<Insert Figure 7 here>>
```{r echo=FALSE, out.width='70%', fig.align='center'}
#knitr::include_graphics('Table of selected variables.png')
```
- Underpinned by theory of *convex duality*

## TT Example: Bayesian Logistic Regression

- Consider Bayesian logistic regression model
$$Y_i|\beta_0....$$
- With priors on the coefficient vector of $\beta$ ~ $N(\mu_\beta,\Sigma_\beta)$
- The likelihood is
$$a$$
and the posterior density of $\beta$ is 
$$a$$
where $p(y|\beta)$ is
$$eq21$$
and the denominator is an intractable integral
- Note $-log(1+e^x)$ are the maxima of a family of parabolas:
$$eq22$$

## TT Example: Bayesian Logistic Regression 

- The previous representation will provide the following lower bound on p(y;beta) 

$$\text{between eq 23 and 24}$$
- which is proportional to the Multivariate Normal
$$eq24$$

## TT Example: Bayesian Logistic Regression  

- The previous equation still contains the vector of unknown variational parameters $\xi$ 
- Set p(y,xi) as close as possible to p(y)
- since p(y,xi) <= p(y) for all xi this reduces to maximizing p(y,xi) which can be solved numerically
$$log\ p(y; \xi) = []$$
- Note: Jaakkola and Jordan derived a simpler and quicker algorithm for maximizing numerically based on Expectation Maximization


