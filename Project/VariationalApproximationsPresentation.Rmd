---
title: "Explaining Variational Approximations"
author: "Ricardo Cortez & Ben Graf"
date: "28 Apr 2021"
output:
  ioslides_presentation: default
  beamer_presentation:
    theme: CambridgeUS
    colortheme: default
    fonttheme: structurebold
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
pacman::p_load(pacman)
```

## Outline

- Introduction
<p>&nbsp;</p>
- Density Transform Approach
<p>&nbsp;</p>
- Tangent Transform Approach
<p>&nbsp;</p>
- Conclusions

## Density Transform Approach

- An approach to approximating intractable posterior densities with better known and easier to deal with densities
- Guided by Kullback-Leibler Divergence
$$\text{formula 2+4+ } p(y) > p(y;q)$$
  - provides a lower bound which can be maximized in order to minimize the k-l divergence between q and p(.|y)
- Two main Transforms
  a. Product Density Transforms (non-parametric)
  b. Parametric Density Transforms (parametric)

## Product Density Transforms

- Suppose $q$ is subject to the product restriction (a) from the previous slide
- It can be shown that the optimal densities satisfy 
$$\text{formula 5}$$
where $E_{-\boldsymbol{\theta}_i}$ denotes the expectation of the density with $q_i$ removed
- This leads to the algorithm on the next slide to solve for the $q_i^*$
- Notes:
  + Can show that convergence to at least local optima guaranteed
  + If conjugate priors used, then the $q_i^*$ updates reduce to updating parameters in a density family
  + Common to monitor convergence using $\text{log }\underline{p}(\boldsymbol{y};q)$

## Product Density Transforms

SHOW ALGORITHM 1

## Connection with Gibbs Sampling

- An alternative expression for the $q_i^*$ is 
$$\text{formula 6 subbing in "rest"}$$
- The distributions $\boldsymbol{\theta}_i|\text{sub in for "rest"}$ are called full conditionals in Markov Chain Monte Carlo
- Gibbs sampling uses repeated draws from these
- In fact, product density transforms are tractable when Gibbs is

## PDT Example 1: Normal Random Sample

- Objective is to approximate Bayesian inference for a random sample from a Normal distribution
$$\text{first equation}$$
with conjugate priors
$$\text{second equation}$$
- The product density transform approximation of $p(\mu,\sigma^2|\boldsymbol{x})$ is 
$$\text{formula 8}$$
- The optimal densities take the form
$$\text{next equation}$$

## PDT Example 1: Normal Random Sample

- The resulting estimates are:
$$q_{\sigma^2}^*(\sigma^2)\text{ is InverseGamma(...)}$$
$$q_{\mu}^*(\mu)\text{ is Normal(...)}$$
and
$$\text{log }\underline{p}(\boldsymbol{x};q) = ...$$
- This leads to the algorithm on the next slide

## PDT Example 1: Normal Random Sample

SHOW ALGORITHM 2

## PDT Example 1: Normal Random Sample

- Upon convergence, the posterior densities are approximated as
$$p(\mu|\boldsymbol{x}) \approx ...$$
$$p(\sigma^2|\boldsymbol{x}) \approx ...$$
- Next slide's plots compare product density variational approximations with exact posterior density
  + Sample size $n=20$ from $N(100,225)$
  + Vague priors chosen: $\mu \sim N(0,10^8),\ \sigma^2 \sim IG(\frac{1}{100},\frac{1}{100})$
  + Initial value $B_{q(\sigma^2)}=1$
  + Convergence very rapid, accuracy quite good

## PDT Example 1: Normal Random Sample

SHOW FIGURE 2
```{r echo=FALSE, out.width='70%', fig.align='center'}
#knitr::include_graphics('Table of selected variables.png')
```

## PDT Example 2: Linear Mixed Model

- Objective is to approximate Bayesian inference for a random sample from a Gaussian linear mixed model
$$\text{formula 11}$$
where 
  + $\boldsymbol{y}$ is $n \times 1$ response, 
  + $\boldsymbol{\beta}$ is $p \times 1$ fixed effects, 
  + $\boldsymbol{u}$ is random effects,
  + $\boldsymbol{X}$ and $\boldsymbol{Z}$ are design matrices, and
  + $\boldsymbol{G}$ and $\boldsymbol{R}$ are covariance matrices
- Conjugate priors are
$$\text{formula 13}$$

## PDT Example 2: Linear Mixed Model

Directed Acyclic Graph (DAG) for Example 2
SHOW FIGURE 3
```{r echo=FALSE, out.width='70%', fig.align='center'}
#knitr::include_graphics('Table of selected variables.png')
```

## PDT Example 2: Linear Mixed Model

- The two-component product transform is
$$\text{formula 14}$$
- This leads to optimal densities
$$q_{\boldsymbol{\beta},\boldsymbol{u}}^*({\boldsymbol{\beta},\boldsymbol{u}})\text{ is a Multivariate Normal density}$$
$$q_{\boldsymbol{\sigma^2}}^*\text{ is a product of $r+1$ Inverse Gamma densities}$$
and
$$\text{log }\underline{p}(\boldsymbol{y};q) = ...$$
- This leads to the algorithm on the next slide

## PDT Example 2: Linear Mixed Model

SHOW ALGORITHM 3

## PDT Example 2: Linear Mixed Model

- Upon convergence, the posterior densities are approximated as
$$p(\boldsymbol{\beta},\boldsymbol{u}|\boldsymbol{y}) \approx ...$$
$$p(\sigma^2_{u1},...,\sigma^2_{ur},\sigma^2_{\varepsilon}|\boldsymbol{y}) \approx ...$$

## PDT Example 2: Linear Mixed Model

- Next slide's plots compare product density variational approximations with exact posterior density
  + Data set is longitudinal orthodontic measurements (Pinheiro & Bates, 2000)
  + Random intercept model:
$$\text{formulas 16}$$
  + Vague priors chosen: $\sigma^2_\beta = 10^8,\ A=B=\frac{1}{100}$
  + Compared against kernel density estimates using 1M MCMC samples
  + Convergence again quite rapid, estimates quite close to MCMC, statistical significance of all parameters

## PDT Example 2: Linear Mixed Model

SHOW FIGURE 4
```{r echo=FALSE, out.width='70%', fig.align='center'}
#knitr::include_graphics('Table of selected variables.png')
```

## Parametric Density Transform with Poisson Example

- Assume q is subject to the parametric restriction from earlier. 
  - mainly q belongs to a specific parametric family  and hope this results in a more tractable approximation to the posterior density
- Poisson Regression with Gaussian Transform Example: 
  - consider the The Bayesian Poisson regression Model: 
  
  $$Y_i|\beta_0...$$
  - The priors on the coefficient vector $\beta$ ~ $N(\mu_\beta,\Sigma_\beta)$ and X is defined as before
  
## Poisson Regression with Gaussian Transform

- The likelihood is: 

$$p(y|\beta)=$$

- This leads to an integral that has no closed form solution (intractable)

$$\text{marginal likelihood}$$

## Poisson Regression with Gaussian Transform

- Take q ~ N()
$$q() = equation$$
- then the lower bound as defined earlier gives explicitly: 
$$eq19$$

- note that from earlier 
$$logp(y) >= log(lowerbound)$$
- the optimal variational parameters can be found through maximizing the inequality above using the Newton-Raphson iteration. This will ensure a minimization of the K.L. Divergence and the most optimal Gaussian Density Transform q* . 

## Tangent Transform Approach
  
- Not all variational Approximation fit into Kullback-Leibler Divergence Framework. 
- Tangent Transforms work with *tangent-type* representations.
- eq 20 
<<Insert Figure 7 here>>
- underpinned by *convex duality* <- outside of scope of paper 

## Bayesian Logistic Regression

- Jaakkola and Jordan (2000) explain that BLR lends itself to tangent transform variational approximations
$$Y_i|\beta_0....$$
- The priors on the coefficient vector $\beta$ ~ $N(\mu_\beta,\Sigma_\beta)$ and X is defined as before
- The Likelihood is: 
$$$$
- and the posterior density of $\beta$ is 
$$$$

## Bayesian Logistic Regression
- where $p(y|\beta)$ is: 

$$eq21$$

- Provides an intractable integral

- note -log(1+e^x) as the maxima of a family of parabolas:
$$eq22 \text{ by Jaakkola and Jordan}$$
- Where $xi$ is $nx1$ vector of variational parameters. 

## Bayesian Logistic Regression 

- The previous representation will provide the following lower bound on p(y;beta) 

$$\text{between eq 23 and 24}$$
- which is proportional to the Multivariate Normal
$$eq24$$

## Bayesian Logistic Regression  

- The previous equation still contains the vector of unknown variational parameters $\xi$ 
- Set p(y,xi) as close as possible to p(y). 
- since p(y,xi) <= p(y) for all xi this reduces to maximizing p(y,xi) which can be solved numerically
$$log\ p(y; \xi) = []$$
- Note: Jaakkola and Jordan derived a simpler and quicker algorithm for maximizing numerically based on Expectation Maximization


