---
title: "Explaining Variational Approximations"
author: "Ricardo Cortez & Ben Graf"
date: "28 Apr 2021"
output:
  beamer_presentation:
    theme: CambridgeUS
    colortheme: default
    fonttheme: structurebold
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
pacman::p_load(pacman)
```

## Outline

- Introduction
<p>&nbsp;</p>
- Density Transform Approach
<p>&nbsp;</p>
- Tangent Transform Approach
<p>&nbsp;</p>
- Conclusions

## Introduction
NEED STUFF HERE! VERY IMPORTANT!

## Density Transform Approach

- Approximates intractable posterior densities with better known and easier to deal with densities
- Guided by Kullback-Leibler divergence
$$\text{formula 2+3+4+ } p(y) > p(y;q)$$
  - provides a lower bound which can be maximized in order to minimize the k-l divergence between q and p(.|y)
- Two main types of restrictions for the $q$ density:
  + Product Density Transforms (non-parametric)
  + Parametric Density Transforms (parametric)

## Product Density Transforms

- Suppose $q$ is subject to the product restriction from the previous slide
- It can be shown that the optimal densities satisfy 
$$\text{formula 5}$$
where $E_{-\boldsymbol{\theta}_i}$ denotes the expectation of the density with $q_i$ removed
- This leads to the algorithm on the next slide to solve for the $q_i^*$
- Notes:
  + Can show that convergence to at least local optima guaranteed
  + If conjugate priors used, then the $q_i^*$ updates reduce to updating parameters in a density family
  + Common to monitor convergence using $\text{log }\underline{p}(\boldsymbol{y};q)$

## Product Density Transforms

SHOW ALGORITHM 1

## Connection with Gibbs Sampling

- An alternative expression for the $q_i^*$ is 
$$\text{formula 6 subbing in "rest"}$$
- The distributions $\boldsymbol{\theta}_i|\text{sub in for "rest"}$ are called full conditionals in Markov Chain Monte Carlo
- Gibbs sampling uses repeated draws from these
- In fact, product density transforms and Gibbs are tractable in the same scenarios

## Product DT Example 1: Normal Random Sample

- Objective is to approximate Bayesian inference for a random sample from a Normal distribution
$$\text{first equation}$$
with conjugate priors
$$\text{second equation}$$
- The product density transform approximation of $p(\mu,\sigma^2|\boldsymbol{x})$ is 
$$\text{formula 8}$$
- The optimal densities take the form
$$\text{next equation}$$

## Product DT Example 1: Normal Random Sample

- The resulting estimates are:
$$q_{\sigma^2}^*(\sigma^2)\text{ is InverseGamma(...)}$$
$$q_{\mu}^*(\mu)\text{ is Normal(...)}$$
and
$$\text{log }\underline{p}(\boldsymbol{x};q) = ...$$
- This leads to the algorithm on the next slide

## Product DT Example 1: Normal Random Sample

SHOW ALGORITHM 2

## Product DT Example 1: Normal Random Sample

- Upon convergence, the posterior densities are approximated as
$$p(\mu|\boldsymbol{x}) \approx ...$$
$$p(\sigma^2|\boldsymbol{x}) \approx ...$$
- Next slide's plots compare product density variational approximations with exact posterior density
  + Sample size $n=20$ from $N(100,225)$
  + Vague priors chosen: $\mu \sim N(0,10^8),\ \sigma^2 \sim IG(\frac{1}{100},\frac{1}{100})$
  + Initial value $B_{q(\sigma^2)}=1$
  + Convergence very rapid, accuracy quite good

## Product DT Example 1: Normal Random Sample

SHOW FIGURE 2
```{r echo=FALSE, out.width='70%', fig.align='center'}
#knitr::include_graphics('Table of selected variables.png')
```

## Product DT Example 2: Linear Mixed Model

- Objective is to approximate Bayesian inference for a random sample from a Gaussian linear mixed model
$$\text{formula 11}$$
where 
  + $\boldsymbol{y}$ is $n \times 1$ response, 
  + $\boldsymbol{\beta}$ is $p \times 1$ fixed effects, 
  + $\boldsymbol{u}$ is random effects,
  + $\boldsymbol{X}$ and $\boldsymbol{Z}$ are design matrices, and
  + $\boldsymbol{G}$ and $\boldsymbol{R}$ are covariance matrices
- Conjugate priors are
$$\text{formula 13}$$

## Product DT Example 2: Linear Mixed Model

- The two-component product transform is
$$\text{formula 14}$$
- This leads to optimal densities
$$q_{\boldsymbol{\beta},\boldsymbol{u}}^*({\boldsymbol{\beta},\boldsymbol{u}})\text{ is a Multivariate Normal density}$$
$$q_{\boldsymbol{\sigma^2}}^*\text{ is a product of $r+1$ Inverse Gamma densities}$$
and
$$\text{log }\underline{p}(\boldsymbol{y};q) = ...$$
- This leads to the algorithm on the next slide

## Product DT Example 2: Linear Mixed Model

SHOW ALGORITHM 3

## Product DT Example 2: Linear Mixed Model

- Upon convergence, the posterior densities are approximated as
$$p(\boldsymbol{\beta},\boldsymbol{u}|\boldsymbol{y}) \approx ...$$
$$p(\sigma^2_{u1},...,\sigma^2_{ur},\sigma^2_{\varepsilon}|\boldsymbol{y}) \approx ...$$

## Product DT Example 2: Linear Mixed Model

- Next slide's plots compare product density variational approximations with exact posterior density
  + Data set is longitudinal orthodontic measurements (Pinheiro & Bates, 2000)
  + Random intercept model:
$$\text{formulas 16}$$
  + Vague priors chosen: $\sigma^2_\beta = 10^8,\ A=B=\frac{1}{100}$
  + Compared against kernel density estimates using 1M MCMC samples
  + Convergence again quite rapid, estimates quite close to MCMC, statistical significance of all parameters

## Product DT Example 2: Linear Mixed Model

SHOW FIGURE 4
```{r echo=FALSE, out.width='70%', fig.align='center'}
#knitr::include_graphics('Table of selected variables.png')
```

## Parametric DT Example: Poisson Regression

- Now assume $q$ is subject to the parametric restriction
  - Belongs to a specific parametric family that (hopefully) results in a more tractable approximation to the posterior density
- Poisson Regression with Gaussian Transform example: 
  - Consider the Bayesian Poisson Regression Model: 
  
  $$Y_i|\beta_0...$$
  - With priors on the coefficient vector of $\beta$ ~ $N(\mu_\beta,\Sigma_\beta)$
  
## Parametric DT Example: Poisson Regression

- The likelihood is

$$p(y|\beta)=$$

- This leads to an integral that has no closed form solution (intractable):

$$\text{marginal likelihood}$$

## Parametric DT Example: Poisson Regression

- Take $q \sim N(\boldsymbol{\mu}_{q(\boldsymbol{\beta})},\boldsymbol{\Sigma}_{q(\boldsymbol{\beta})})$
$$q(\boldsymbol{\beta;\mu}_{q(\boldsymbol{\beta})},\boldsymbol{\Sigma}_{q(\boldsymbol{\beta})}) =

(2\pi)^{-p/2}|\boldsymbol{\Sigma}_{q(\boldsymbol{\beta})}|^{-1/2}*exp\{-\frac{1}{2}(\boldsymbol{\beta-\mu}_{q(\boldsymbol{\beta})})^T \boldsymbol{\Sigma}_{q(\boldsymbol{\beta})}^{-1} (\boldsymbol{\beta-\mu}_{q(\boldsymbol{\beta})})\}

$$
- Then the lower bound as defined earlier gives explicitly
$$

log \underline{p}(\boldsymbol{y; \mu_{q(\beta)}, \Sigma_{q(\beta)}}) = 


\boldsymbol{y}^T\boldsymbol{X\mu_{q(\beta)}-I}_n^Texp\{\boldsymbol{X\mu_{q(\beta)}}+\frac{1}{2}diagonal(\boldsymbol{X\Sigma}_{q(\beta)}\boldsymbol{X}^T)\}


-\frac{1}{2}(\boldsymbol{\mu_{q(\beta)}-\mu_\beta})^T\boldsymbol{\Sigma}^{-1}_\beta(\boldsymbol{\mu}_{q(\boldsymbol{\beta})}-\mu_\boldsymbol{\beta})-\frac{1}{2}tr(\boldsymbol{\Sigma}^{-1}_\boldsymbol{\beta}\boldsymbol{\Sigma}_{q(\boldsymbol{\beta})})

+\frac{1}{2}log|\boldsymbol{\Sigma}_{q(\boldsymbol{\beta})}|-\frac{1}{2}log|\boldsymbol{\Sigma_\beta}|+\frac{k+1}{2}-\boldsymbol{I}^T_nlog(\boldsymbol{y!})

$$

- From earlier,
$$logp(y) \geq log \underline{p}(\boldsymbol{y; \mu_{q(\beta)}, \Sigma_{q(\beta)}})$$
- The optimal variational parameters are found through maximizing this inequality using Newton-Raphson iteration
- This minimizes the K-L divergence and provides the optimal Gaussian density transform $q^*$

## Tangent Transform Approach
  
- Not all variational approximations fit into Kullback-Leibler divergence framework
- Tangent transforms work with *tangent-type* representations of concave/convex functions
$$log(x) = min\{\xi x -log(\xi)-1\} \text{ for all x> 0}$$

```{r echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics('Figure7.png')

```
- Underpinned by theory of *convex duality*

## TT Example: Bayesian Logistic Regression

- Consider Bayesian logistic regression model
$$Y_i|\beta_0,...,\beta_k\sim Bernoulli([1+exp\{-(\beta_0+\beta_1x_{1i}+...+\beta_kx_{ki}\}]^{-1}), \text{ }1\leq i \leq n$$

- With priors on the coefficient vector of $\beta \sim N(\mu_\beta,\Sigma_\beta)$
- The likelihood is
$$p(\boldsymbol{y|\beta)}=exp[\boldsymbol{y^TX\beta-I^T_n}log\{I_n+exp(\boldsymbol{X\beta})\}]$$
and the posterior density of $\beta$ is 
$$p(\boldsymbol{\beta|y})=p(\boldsymbol{y,\beta})/ \int_ {\mathbb{R}^{k+1}}p(\boldsymbol{y,\beta)d\beta}$$
where 
$$p(y|\beta) = exp[\boldsymbol{y}^T\boldsymbol{X\beta - I_n^T}log(\boldsymbol{I_n}+exp(\boldsymbol{X\beta}))-\frac{1}{2}(\boldsymbol{\beta-\mu_\beta})^T\boldsymbol{\Sigma^{-1}_\beta(\beta-\mu_\beta)
}-\frac{k+1}{2}log(2\pi)-\frac{1}{2}log|\boldsymbol{\Sigma_\beta}|]$$
and the denominator is an intractable integral


## TT Example: Bayesian Logistic Regression 

- It can be shown $-log(1+e^x)$ are the maxima of a family of parabolas:
$$-log(1+e^x)=max\{A(\xi)x^2-\frac{1}{2}x+C(\xi)}$$

$$A(\xi)=-tanh(\xi/2)/(4\xi)$$ 

and

$$C(\xi)=\xi/2-log(1+e^\xi)+\xi tanh(\xi/2)/4$$


- above equation is a *tangent-type* representation of a convex function. 

## TT Example: Bayesian Logistic Regression 

- Under similar derivations as the previous problem, the family of variational approximations to $\beta|y$ is:
$$\boldsymbol{\beta|y;\xi\sim}N(\mu(\xi),\Sigma(\xi))$$

where
$$\boldsymbol{\Sigma(\xi)=}[\boldsymbol{\Sigma^{-1}_\beta-2X^T}diag{A(\xi)\boldsymbol{X}}]^{-1}$$
and
$$\boldsymbol{\mu(\xi)=\Sigma(\xi)\{{X^T(y-\frac{1}{2}I)+\Sigma^{-1}_\beta\mu_\beta}}\}$$

- Note: Jaakkola and Jordan derived a simpler and quicker algorithm for maximizing numerically based on Expectation Maximization



## Conclusion

- Doesn't cover the accuracy of the approximations
- Usefulness of the variational approximations increases as the size increases
  - MCMC begins to become untenable